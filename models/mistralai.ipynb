!pip install -q transformers accelerate sentencepiece bitsandbytes
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
from huggingface_hub import login
from google.colab import userdata
login(userdata.get("token_hf")) 
model_id = "mistralai/Mistral-7B-Instruct-v0.2"
tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    device_map="auto",
    torch_dtype="auto",
)
def generate_emotion(text):
    prompt = f"""<s>[INST] Detect and list the emotional categories expressed in the following text. Output as a JSON with emotion names and scores between 0 and 1.
 Text: "{text}"
Output: [/INST]"""
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    output = model.generate(
        **inputs,
        max_new_tokens=200,
        do_sample=True,
        top_p=0.95,
        temperature=0.7,
        pad_token_id=tokenizer.eos_token_id,
    )
     decoded = tokenizer.decode(output[0], skip_special_tokens=True)
    return decoded.split("Output:")[-1].strip()
print("\n Emotions :\n", generate_emotion(input()))
