# Step 1: Upload file from your PC to Google Colab
from google.colab import files
uploaded = files.upload()

# After uploading, get the filename (assuming only one file uploaded)
import io
import pandas as pd

filename = next(iter(uploaded))
df = pd.read_csv(io.BytesIO(uploaded[filename]))

# Step 2: Data preprocessing

# Import necessary libraries
import re
import string
from sklearn.utils import shuffle

# Function to remove emojis from text
def remove_emoji(text):
    emoji_pattern = re.compile("["
        u"\U0001F600-\U0001F64F"  # emoticons
        u"\U0001F300-\U0001F5FF"  # symbols & pictographs
        u"\U0001F680-\U0001F6FF"  # transport & map symbols
        u"\U0001F1E0-\U0001F1FF"  # flags
        u"\U00002700-\U000027BF"  # Dingbats
        u"\U000024C2-\U0001F251"
        "]+", flags=re.UNICODE)
    return emoji_pattern.sub(r'', text)

# Function to clean text: remove emojis, punctuations, special chars
def clean_text(text):
    text = str(text)
    text = remove_emoji(text)  # remove emojis
    text = text.lower()  # lowercase
    # Remove punctuation and special characters except spaces
    text = re.sub(r'[^a-z0-9\s]', '', text)
    # Remove extra whitespace
    text = re.sub(r'\s+', ' ', text).strip()
    return text

# Apply text cleaning on the 'Text' column
df['Text'] = df['Text'].apply(clean_text)

# Step 3: Remove unwanted columns
# Assuming the relevant columns for sentiment classification are 'Text' and 'Sentiment'
columns_to_keep = ['Text', 'Sentiment']
df = df[columns_to_keep]

# Step 4: Balance the dataset
# Using imblearn's RandomOverSampler to balance classes by upsampling minority classes
from imblearn.over_sampling import RandomOverSampler

X = df['Text']
y = df['Sentiment']

# Reshape X for compatibility (imblearn expects 2D array)
X = X.values.reshape(-1, 1)

ros = RandomOverSampler(random_state=42)
X_resampled, y_resampled = ros.fit_resample(X, y)

# Convert back to DataFrame
df_balanced = pd.DataFrame({
    'Text': X_resampled.flatten(),
    'Sentiment': y_resampled
})

# Shuffle the balanced dataset
df_balanced = shuffle(df_balanced, random_state=42).reset_index(drop=True)

# Display some info about the balanced dataset
print("Class distribution after balancing:")
print(df_balanced['Sentiment'].value_counts())

print("\nSample preprocessed data:")
print(df_balanced.head())

!pip install transformers datasets evaluate

import pandas as pd
from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer
from datasets import Dataset
import numpy as np
import evaluate


# Define the model and tokenizer
model_name = "nlptown/bert-base-multilingual-uncased-sentiment" #Example, try others for better results
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=len(df_balanced['Sentiment'].unique())) # Ensure correct num_labels

# Tokenize the data
def tokenize_function(examples):
    return tokenizer(examples["Text"], padding="max_length", truncation=True)


# Create Hugging Face Dataset
dataset = Dataset.from_pandas(df_balanced)
tokenized_datasets = dataset.map(tokenize_function, batched=True)

# Split the data into train and test sets
train_testvalid = tokenized_datasets.train_test_split(test_size=0.2)
test_valid = train_testvalid['test'].train_test_split(test_size=0.5)
train_dataset = train_testvalid['train']
test_dataset = test_valid['test']
valid_dataset = test_valid['train']


# Define the metric
metric = evaluate.load("accuracy")

def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)
    return metric.compute(predictions=predictions, references=labels)


# Define the training arguments
training_args = TrainingArguments(
    output_dir="./results",
    evaluation_strategy="epoch",
    save_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=16, 
    per_device_eval_batch_size=16, 
    num_train_epochs=3,              
    weight_decay=0.01,
    load_best_model_at_end=True,
    metric_for_best_model='accuracy',
)


# Create the Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=valid_dataset,
    compute_metrics=compute_metrics,
)

# Train the model
trainer.train()


# Evaluate the model
trainer.evaluate(eval_dataset=test_dataset)

# Save the trained model (optional)
trainer.save_model("./sentiment_analysis_model")
